\documentclass[a4paper]{llncs}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb,amsfonts,amsmath}
\usepackage{graphicx}
\usepackage{alltt}
\usepackage{multicol}
%\usepackage[pdftex]{pstricks}
\usepackage{verbatim}
%\usepackage[font=small,labelfont=bf]{caption}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{xcolor}
\usepackage{synttree}
  \branchheight{0.3in}
  \childattachsep{1em}
  \childsidesep{1em}
%\usepackage[pdftex,colorlinks=true,plainpages=false]{hyperref}

%\usepackage[subtle,leading]{savetrees}
%\usepackage[subtle]{savetrees}
%\usepackage[moderate]{savetrees}

\newcommand{\url}[1]{{\tt #1}}

% Various useful commands for revisions...
\newcommand{\KILL}[1]{}
\newcommand{\HIDE}[1]{}

%\newtheorem{algorithm}{Algorithm}
\newcommand{\assign}{\leftarrow}
\renewcommand{\algorithmiccomment}[1]{~~// #1}

% abstract syntax
\newenvironment{datatype}{$\begin{array}{lcl}}{\end{array}$}
\newenvironment{grammar}{$\begin{array}{lcl}}{\end{array}$}
\newcommand{\is}{&\ ::=\ &}
\newcommand{\gen}{&\ \rightarrow\ &}
\newcommand{\alter}{\ \mid\ }
\newcommand{\altis}{\\ &\ \mid\ &}
%\newenvironment{action}{ & & \left\{\begin{array}{lcl}}{\end{array}\right.}
\newcommand{\laction}{\quad \{\ }
\newcommand{\baction}{\\ & & \left\{\begin{array}{lcl}}
\newcommand{\eaction}{\end{array}\right.}
\newcommand{\defby}{& := &}
\newcommand{\acode}[1]{\textrm{{\tt #1}}}
\newcommand{\concat}{~@@~}
\newcommand{\then}{\\ & @@ &}
\newcommand{\dtskip}{\end{array}$ $\begin{array}{lcl}}
\newcommand{\mathsc}[1]{\textrm{\sc #1}}

\newcommand{\nat}{\mathbb{N}}

\sloppy

%\pagestyle{plain}

\begin{document}

\title{An MDL-based Approach to the ARC Challenge}

\author{Sébastien Ferré}

\institute{Univ Rennes, CNRS, IRISA\\ %, Universit\'e de Rennes 1\\
  Campus de Beaulieu, 35042 Rennes, France\\
  Email: \email{ferre@irisa.fr}}

\maketitle

\begin{abstract}
\end{abstract}

%% keywords
\HIDE{
Measure of Intelligence
Artificial Intelligence
Explainable AI
Program Synthesis
Structured Prediction
Minimum Description Length
2D Parsing
}

\sloppy

\section{Introduction}
\label{intro}

This document describes our approach to the ARC (Abstraction and
Reasoning Corpus) challenge introduced by François Chollet as a tool
to measure intelligence, both human and artificial~\cite{Chollet2019}.
ARC was introduced in order to foster research on Artificial General
Intelligence (AGI)~\cite{Goertzel2014agi}, and to move from
system-centric generalization to developer-specific
generalization. The latter enables a system to ``handle situations
that neither the system nor the developer of the system have
encountered before''. For instance, a system trained to recognize cats
features system-centric generalization because it can recognize cats
that it has never seen, but in general, it does not feature
developer-aware generalization because it can not learn to recognize
horses with only a few examples (unlike a young child would).

ARC is made of 1000 learning tasks, 400 for training, 400 for
evaluation, and 200 kept secret by its author in order to ensure
bias-free evaluation. Each task consists in learning how to generate
an output colored grid from an input colored grid (see
Figure~\ref{fig:task} for an example).
%
ARC is a challenging target for AI. As F. Chollet writes: ``to the
best of our knowledge, ARC does not appear to be approachable by any
existing machine learning technique (including Deep
Learning)''~\cite{Chollet2019}. The main difficulties are the
following:
\begin{itemize}
\item The expected output is not a label, or even a set of labels, but
  a colored grid with size up to 30x30, and with up to 10 different
  colors. It therefore falls in the domain of {\em structured
    prediction}~\cite{dietterich2008structured}.
\item The predicted output has to match exactly the expected
  output. If a single cell is wrong, the task is considered as
  failed. To compensate for that, three attempts are allowed for each
  input grid.
\item In each task, there are generally between 2 and 4 training
  instances (input grid + output grid), and 1 or 2 test instances for
  which a prediction has to be made.
\item Each task relies on a distinct transformation from the input
  grid to the output grid. In particular, no evaluation task can be
  solved by reusing a transformation learned on the training
  tasks. Actually, each task is a distinct learning problem, and what
  ARC evaluates is broad generalization and few-shot learning.
\end{itemize}
A compensation for those difficulties is that the the input and output
data are much simpler than real-life data like photos or texts. We
think that this helps to focus on the core features of intelligence
rather than on scalability issues.

Our approach is based on the MDL (Minimum Description Length)
principle~\cite{Rissanen1978,Grunwald2019} according to which {\em ``the best
model for some data is the one that compresses the most the data''}.
%
The MDL principle has already been applied successfully to pattern
mining, classification, and clustering tasks~\cite{KRIMP2011}.
%
A major difference with those work is that the model to be learned
needs not only explain the data but also be able to perform a
prediction, here the output grid from the input grid.
%
At this stage (Version~2), we consider rather simple models that have
no chance to cover a large proportion of ARC tasks: single-color
points and shapes, and basic arithmetic of positions and sizes. Our
prime objective is to demonstrate the effectiveness of an MDL-based
approach to the ARC challenge, and hopefully to AGI. Our approach
succeeds on 25/400 training tasks and 6/400 evaluation tasks, with a
learning timeout of 1min only per task. We find such a result quite
encouraging given the difficulty of the problem. Moreover, the
successful tasks are quite diverse, and the learned model is often
close to the simplest model for the task (in the chosen class of
models).

This report is organized as follows. Section~\ref{arc} gives a formal
definition of ARC grids and tasks. Section~\ref{related} discusses
related work on artificial intelligence, in particular structured
prediction, AGI, or programming by demonstration; it also presents the
MDL principle and some of its applications to AI and knowledge
discovery. Section~\ref{modelling} explains our modelling of the ARC
tasks in the framework of two-parts MDL, in particular our class of
models. Section~\ref{learning} describes the MDL-based learning
process, which boils down to defining description lengths and model
refinements. Section~\ref{eval} reports on the evaluation of our
approch on training and evaluation ARC tasks, as well as on the
learned models for a few tasks.

%%%
\section{Defining ARC Tasks}
\label{arc}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{fig/training_b94a9452.png}
  \caption{One of the training tasks ({\tt b94a9452}), with three
    examples on the left, a test input grid in the middle, and the
    widget on the right that can be used to draw the missing output
    grid.}
  \label{fig:task}
\end{figure}

\begin{definition}[colors]
  We assume a finite set~$C$ of distinct symbols, which we call {\em colors}.
\end{definition}

In ARC, there are 10 colors coded by digits (0..9) in JSON files, and
by colors (e.g., black, purple, red) in the web interface (see
Figure~\ref{fig:task}).

\begin{definition}[grid]
  A {\em grid} $g \in C^{h \times w}$ is a matrix of colors with $h>0$
  rows, and $w>0$ columns. A grid is often displayed as a colored
  grid. The number of rows~$h = {\it height}(g)$ is called the {\em
    height} of the grid, and the number of
  columns~$w = {\it width}(g)$ is called the {\em width} of the grid.
  
  A {\em grid cell} is caracterized by {\em coordinates}~$(i,j)$,
  where $i$ selects a row, and $j$ selects a column. The color at
  coordinates~$(i,j)$ is denoted either by~$g_{ij}$ or by~$g[i,j]$.
  Coordinates range from~$(0,0)$ to~$(h-1,w-1)$.
\end{definition}

In ARC, grids have a size from (1,1) to (30,30).

\begin{definition}[example] 
  An {\em example} is a pair of grids~$e = (g^i,g^o)$, where $g^i$ is called
  the {\em input grid}, and $g^o$ is called the {\em output grid}.
\end{definition}

As illustrated by Figure~\ref{fig:task}, the output grid needs not
have the same size as the input grid, it can be smaller or bigger.

\begin{definition}[task]
  A {\em task} is a pair~$T = (E,F)$, where $E$ is the set of {\em
    train examples}, and $F$ is the set of {\em test examples}.
\end{definition}

The learner has only access to the input and ouput grids of the
train examples. Its objective is, for each test example, to predict
the output grid given the input grid.
%
As illustrated by Figure~\ref{fig:task}, the different input grids of
a task need not have the same size, nor use the same colors. The same
applies to test grids.

The ARC is composed of 1000 tasks in total: 400 for training, 400 for
evaluation by developers, and 200 for independent
evaluation. Figure~\ref{fig:task} shows one of the 400 training tasks,
with three train examples, and one test input grid. Developers should
only look at the training tasks, not at the evaluation tasks, which
should only be used to evaluate the broad generalization capability of
the system.
%
Each task has generally between 2 and 4 train examples, and 1 or 2
test examples.

%%%
\section{Related Work}
\label{related}

AI, machine learning, structured prediction...

Programming by demonstration...

MDL work...



%%%
\section{Modelling ARC Tasks in Two-Parts MDL}
\label{modelling}

The objective is here to design a class of {\em task models} that can
be used to represent a transformation from input grids to output
grids.
%
Our key intuition is that a task model can be decomposed into two grid
models, one for the input grids, and another for the output grids. The
{\em input grid model} expresses what all train input grids have in
common, while the {\em output grid model} expresses what all train
output grids have in common, as a function of the input grid.
%
The actual transformation from input grid to output grid involves the
two models as follows. The input grid model is used to {\em parse} the
input grid, which produces some information that characterizes the
input grid. Then, that parsing information is fed into the output grid
model in order to generate an output grid.

\subsection{Models}
\label{model}

\begin{figure}[t]
\begin{center}
\begin{datatype}
  {\it Pair} \is {\bf InOut}(in: {\it Grid}, out: {\it Grid})
  \\
  {\it Grid} \is {\bf Grid}(size: {\it Vector},\ color: C,\ layers: list({\it Shape}))
  \\
  {\it Shape} \is {\bf Point}(pos: {\it Vector},\ color: C)
  \altis {\bf Rectangle}(pos: {\it Vector},\ size: {\it Vector},\ color: C,\ mask: {\it MK})
  \\
  {\it Vector} \is {\bf Vec}(i: \nat,\ j: \nat)
\end{datatype}
\end{center}
\caption{Datatypes used in grid models and grid data}
\label{fig:patterns}
\end{figure}

\paragraph{Version 2.}
The main component of our grid models is {\em shapes}. Indeed, ARC
grids can often be read as various shapes over a background. Shapes
can have nested shapes, be adjacent or overlap with other shapes. Each
shape has a number of attributes (e.g., height, width, position, and
color for rectangles; position and color for points). Each shape
attribute may be constant (e.g., the shape is always red) or variable
across the examples.

\KILL{Given a shape, parsing a grid consists in finding its instances in the
grid, if any, and to return corresponding values for its variable
attributes. In contrast, generating a grid requires values for the
variable attributes as input in order to generate a shape instance in
the grid.}

We define our models as templates over data structures representing
concrete pairs of grids. Figure~\ref{fig:patterns} defines those data
structure with algebraic data types for pairs, grids, shapes, and
vectors (2D positions and sizes). They use primitive types for natural
numbers ($\nat$), colors ($C$), and 2D bitmaps we call {\em masks}
(${\it MK}$). Uppercase names in bold are called {\em constructors}, and the
lowercase names of constructor arguments are called {\em fields}.

A task is made of two grids, one for the input grid, and another for
the output grid.
%
Each grid is described as a stack of layers on top of a background,
each layer being made of a single shape. The background has a size (a
2D vector), and a color.
%
A shape is either a point, described by its position and color; or a
rectangle, described by its position, size, color, and mask. That mask
allows to account for irregular shapes, and indicates which pixels in
the rectangle box actually belong to the shape (other pixels can be
seen as transparent).
%
A data structure describing the first train pair in Figure~\ref{fig:task}
is:
\[\begin{array}{l}
    {\bf InOut}( \\
    \quad {\bf Grid}({\bf Vec}(12,13), black,\\
    \quad \quad [{\bf Rectangle}({\bf Vec}(2,4), {\bf Vec}(2,2), yellow, full),\\
    \quad \quad ~{\bf Rectangle}({\bf Vec}(1,3), {\bf Vec}(4,4), red, full)]), \\
    \quad {\bf Grid}({\bf Vec}(4,4), yellow, \\
    \quad \quad [{\bf Rectangle}({\bf Vec}(1,1), {\bf Vec}(2,2), red, full)])).
  \end{array} \]
Mask $full$ denotes a mask where all pixels in the rectangle box are on.

A {\em path} in a data structure is a sequence of fields that goes
from the root of the data structure to some sub-structure. In the
above example, path~$in.size.j$ refers to the input grid width, which
is $13$; path~$out.layers[0].color$ refers to the color of the top
shape in the output grid. which is $red$; and path~$in.layers[1].pos$
refers to the position of the bottom shape in the input grid, which is
${\bf Vec}(1,3)$. Given a data structure~$d$ and a path~$p$, the
substructure of~$d$ that is refered by~$p$ is written with the
dot-notation $d.p$.

In order to generalize from specific pairs of grids to general task
models, we introduce two kinds of elements that can be used in place
of any sub-structure: unknowns and expressions.
%
An {\em unknown}, which we note with a question mark $?$, indicates
that any data structure of the expected type can fit in. For example,
a grid model that matches all input grids in Figure~\ref{fig:task} is:
\[\begin{array}{l}
    {\bf Grid}({\bf Vec}(12,?), black,\\
    \quad [{\bf Rectangle}(?, ?, ?, ?),\\
    \quad ~{\bf Rectangle}(?, ?, ?, full)])
  \end{array}\]
%
It matches grids with 12 rows and a black background, and two stacked
rectangles of any position, any size, and any color. The top rectangle
may have any mask, while the bottom rectangle is full. The purpose of
parsing is to fill in the holes, replacing unknowns with data
structures such that the whole resulting data structure correctly
describes a grid or a pair of grids.
%
An {\em unknown path} is a path in the model that leads to an
unknown. The unknown paths in the above grid model are: $size$,
$layers[0].pos$, $layers[0].size$, $layers[0].color$, $layers[1].pos$,
$layers[1].size$, $layers[1].color$. The set of unknown paths of a
model~$M$ is denoted by~$U(M)$.
%
A data structure that may contain unknowns is called a {\em
  pattern}. A data structure is said {\em ground} if it contains no
unknown. Given a ground data structure~$d$ and a pattern~$p$, the
notation $d \sim p$ says that $d$ {\em agrees with}~$p$ or that $p$
{\em matches}~$d$. It means that the ground data structure~$d$ can be
obtained by a substitution of the pattern unknowns with ground data
structures. For example, we have
${\bf Vec}(12,14) \sim {\bf Vec}(12,?)$.

An {\em expression} defines part of the data structure as the result
of a computation over some input data structure, which we call the
{\em environment}. Expressions are made of primitive values,
constructors, operators/functions, and variables. In Version~2, the
only operators are addition and substraction over natural numbers,
they are used to compute the position and size of grids and shapes.
Variables are references to parts of the environment. A convenient way
to refer to such parts is to use {\em paths} in the environment data
structure.
%
Expressions that would only be made of primitive values and
constructors would actually be data structures, and are therefore not
considered as expressions.
%
The environment of the output grid is the input grid, so that parts of
the output grid can be computed based on the input grid contents. The
input grid has no environment so that expressions are not useful in
the input grid model.
%
Re-using the above model for input grids, we can now define a correct
and complete model for the task in Figure~\ref{fig:task}, using
unknowns for the input grid, and expressions for the output grid:
\[\begin{array}{l}
    {\bf InOut}( \\
    \quad {\bf Grid}({\bf Vec}(12,?), black,\\
    \quad \quad [~{\bf Rectangle}({\bf Vec}(?,?), {\bf Vec}(?, ?), ?, full),\\
    \quad \quad ~~{\bf Rectangle}({\bf Vec}(?,?), {\bf Vec}(?, ?), ?, full)~]), \\
    \quad {\bf Grid}(layers[1].size, layers[0].color, \\
    \quad \quad [~ {\bf Rectangle}(\\
    \quad \quad \quad {\bf Vec}(layers[0].i - layers[1].i, layers[0].j - layers[1].j), \\
    \quad \quad \quad layers[0].size, layers[1].color, full) ~])).
  \end{array} \]

In words, this models says: ``find two stacked full rectangles on a
black background in the input grid, then generate an output grid,
whose size is the same as the bottom shape, and whose background color
is the color of the top shape, and finally add a full rectangle shape
whose size is the same as the top shape, whose color is the same as
the bottom shape, and whose position is the difference between the top
shape position and the bottom shape position (relative position).''

A model is {\em well-formed} if it is well-typed (constructor
arguments, operator operand types, etc,), and if all variables are
valid paths in the environment data. More specifically, a task model
is well-formed if the input grid model contains no variable, and if
all variables in the output grid model are valid paths in the input
grid model.

A task model is {\em definite} if its output grid model contains no
unknown, so that it can be used to deterministically generate an
output grid from a data structure describing the input grid.


\subsection{Data According to a Model}
\label{data}

In MDL-based approaches, it is essential to have a lossless encoding
of data in order to have a fair comparison of different
models. Therefore, it is important to identify the ``data according to
the model'', i.e. the data that needs to be added to a model in order
to capture all the information in the original data.

We first consider grid models, where data is a grid. An analogy can be
made with formal languages and grammars. In this analogy, grids are
sentences, grid models are grammars, and data according to the model
is the parse tree. A parse tree explains how the grammar generates
the sentence. Here, data structures as defined in
Figure~\ref{fig:patterns} play the role of parse trees for grid
models, they are {\em grid parse trees} and we denote them with the
greek letter~$\pi$. A grid parse tree, the data according to the
model, is a data structure that instantiates the grid model by
replacing expressions by their value, and unknowns by values such that
the resulting data structure correctly describes the grid.

For input grid~$g^i_k$ ($k$-th example), $\pi^i_k$ denotes one
possible grid parse tree according to some fixed input grid
model~$m^i$. Similarly, for output grid~$g^o_k$, $\pi^o_k$ denotes one
possible grid parse tree according to some fixed output grid
model~$m^o$ and output environment~$\varepsilon^o$.

In practice, a grid model may not explain all the cells of a
grid. This happens in case of noise in the data, and also in the
learning process where intermediate models are obviously
incomplete. To account for those situations without losing
information, we define ``data according to the model'' as a grid parse
tree plus the list of cells that differ between the original grid~$g$
and the grid generated by the grid parse tree~$\pi$. We call this
additional information a {\em grid delta}, written~$\delta$.
\KILL{Formally, a grid
delta is a set of triples~$(i,j,c) \in \nat \times \nat \times C$
stating that the cell at coordinates~$(i,j)$ has color~$c$ in~$g$,
which is different from the color specified by~$\pi$.}

An additional difficulty is when a test input grid does not match the
input grid model learned from the train examples. For instance, all
input grids in the train examples have size (10,10) but the test
input grid has size (10,12). In this case, there is no grid parse tree
that agrees with both the grid model and the grid. In many cases, this
discrepancy does not alter the validity of the model, which only
happens to be overspecific (a form of overfitting).
%
Our approach to this difficulty is to allow for approximate parsing,
i.e. to allow grid parse trees to diverge from the grid model. As
those differences are present in the grid parse tree~$\pi$, there is
no loss of information. Of course, the description length will take
into acount such differences to penalize them.

As an illustration, let us consider the following (incomplete) input
grid model for the task in Figure~\ref{fig:task}:
\[ {\bf Grid}(?, black, [\ {\bf Rectangle}(?,?,?,full)\ ]). \] There
are mainly two ``data according to the model'' for the first input
grid~$g^i_1$ (the upper index is $i$ for input grids and $o$ for
output grids, the lower index identifies the example):
\begin{enumerate}
\item matching the outer red rectangle\\
  $\pi^i_1 = {\bf Grid}({\bf Vec}(12,13), black, [\ {\bf Rectangle}({\bf Vec}(1,3), {\bf Vec}(4,4), red, {\it full})\ ])$\\
  $\delta^i_1 = \{ (2,4,yellow), (2,5,yellow), (3,4,yellow), (3,5,yellow) \}$

\item matching the inner yellow rectangle\\
  $\pi^i_1 = {\bf Grid}({\bf Vec}(12,13), black, [\ {\bf Rectangle}({\bf Vec}(2,4), {\bf Vec}(2,2), yellow, {\it full})\ ])$\\
  $\delta^i_1 = \{ (1,3,red), (1,4,red), (1,5,red), (1,6,red), (2,3,red), (2,6,red), (3,3,red), ...\}$
\end{enumerate}
The first solution looks better because it has a much smaller grid
delta, and its grid parse tree has the same size as in the second
solution.


\subsection{Primitive Functions on Grids, Grid Models, Grid Parse Trees, and Grid Deltas}
\label{primitive:functions}

We here define the key primitive functions to manipulate grids, grid
models, grid parse trees and grid deltas.

We first define {\em grid deltas} and simple arithmetics on them.

\begin{definition}[grid delta]
  Let $g_1, g_2 \in C^{h \times w}$ two same-size grids. The {\em grid
    delta} $\delta \subseteq \nat \times \nat \times C$ between the
  two grids, written $g_2 - g_1$, is defined as the set of cells that
  need changing color in~$g_1$ in order to obtain~$g_2$.
  \[ \delta = g_2 - g_1 = \{ (i,j,c) \mid g_2[i,j] \neq g_1[i,j], c = g_2[i,j] \} \]
  
  A grid delta~$\delta$ can be added to a grid~$g_1$ to get a corrected same-size grid~$g_2$.
  \[ g_2 = g_1 + \delta \iff \forall i \in [0,h[, j \in [0,w[: g_2[i,j] =
    \left\{
      \begin{array}{ll}
        c & \quad\textrm{if~} (i,j,c) \in \delta \\
        g_1[i,j] & \quad\textrm{otherwise} \\
      \end{array}\right. \]
  %% lemma: g_1 + (g_2 - g_1) = g_2
\end{definition}

Then we define two functions that are directly related to the
semantics of grid models. The first function converts a grid parse tree
into a grid.

\begin{definition}[grid drawing]
  Let $\pi$ be a grid parse tree, ${\it draw}(\pi)$ is the grid that
  results from the drawing following the instructions given by the
  grid parse tree.  
\end{definition}

\paragraph{Version 2.} Given a grid parse tree~$\pi$ as specified in
Figure~\ref{fig:patterns}, the grid ${\it draw}(\pi)$ is obtained by
first generating a grid with the specified size and background color,
and then layers are drawn on that grid, bottom up. Each layer, either
a point or a rectangle, is drawn in the obvious way. For a point
${\bf Point}({\bf Vec}(i,j),c)$, the cell at position~$(i,j)$ is set
to color~$c$. For a rectangle
${\bf Rectangle}({\bf Vec}(i,j),{\bf Vec}(h,w),c,m)$, each cell at
position $(i+x,j+y)$, for any $x \in [0,h[$ and $y \in [0,w[$, is set
to color~$c$ if the relative position~$(x,y)$ belongs to mask~$m$.

\paragraph{}The second function applies a grid model to an
environment, resolving references to it, and replacing expressions by
their evaluation result.

\begin{definition}[model application]
  Let $m$ be a grid model, and $\varepsilon$ an environment for the
  model. ${\it apply}(m,\varepsilon)$ is the model that results from
  the substitution of expressions in~$m$ by their evaluation over the
  environment~$\varepsilon$. The resulting model has no expression but
  may still have unknowns.
  %
  The operation is ill-defined if some variable in~$m$ is not defined
  in~$\varepsilon$. 
\end{definition}

\paragraph{Version 2.} Environments are grid parse trees, and
variables are paths into them. There are only two operators, plus and
minus, and they are evaluated in the usual way on integer values.

\paragraph{}
We also define two non-deterministic functions that produce grid parse
trees from a grid model, and hence grid descriptions. The first
function corresponds to the parsing of a grid.

\begin{definition}[grid parsing]
  Let $m$ be a grid model, and $g$ be a grid. ${\it parse}(m,g)$
  non-deterministically returns a grid parse tree~$\pi$ that
  approximately agrees with model~$m$, and approximately draws as
  grid~$g$. Non-determinism allows for different interpretations of
  the grid by the model.
\end{definition}

\paragraph{Version 2.} This is by far the most complex primitive
function.
%
Parsing is decomposed in three stages. First, the grid is partitioned
into contiguous monocolor regions.
%
Second, shapes are looked for as single parts or as unions of parts to
cover the case where some shapes are overlaped by other shapes. When a
shape is not a point or a full rectangle, two shapes are considered:
one shape with a full mask, considering the missing cells as noise on
top of the rectangle; another shape with a mask containing the cells
in the rectangle box that have the same color. 
%
Third, matching superpositions of shapes are looked for according to
the model. Candidate shapes are considered in a fixed ordering that
includes some heuristics. Black shapes are considered last because
black is often the background color. Then, shapes are considered in
dereasing number of covered cells. Beyond those heuristics, a total
ordering is defined because it makes experiments more reproducible,
and also because it helps disambiguate some situations. For instance,
if the model contains several point shapes, they will be matched to
points in the grid from left to right, top-down.

As an example of parsing, the test input grid in Figure~\ref{fig:task}
has three contiguous parts: the light blue one, the green one, and the
black one. Three rectangles can be identified from those parts: a
small light blue square, a green rectangle overlaped by the blue one,
and a black rectangle covering the whole grid and overlaped by the
other rectangles. According to the above model, this input grid is
parsed as a 2x2 light blue rectangle at coordinates (3,4), on top of a
6x6 green rectangle at coordinates (1,2), on top of a black
background.

Compared to Version~1, parsing has been made non-deterministic in the
sense that a sequence of grid parse trees is produced. We use the
description lengths as defined below to order this sequence from the
shorter DL to the longer DL. Indeed, the shorter the DL, the more
plausible the grid parse tree is. Because of combinatorial problems in
the parsing process, we use various bounds: maximum number of concrete
shapes for a given shape in the model, maximum number of produced grid
parse trees before sorting them, and maximum number of grid parse
trees actually returned. Relative to approximate parsing, we also
bound the number of differences w.r.t. the model.

\paragraph{}
The second function corresponds to generation, where the grid parse
tree is produced by filling in the holes of the grid model,
i.e. guessing at the missing elements.

\begin{definition}[grid generation]
  Let $m$ be a grid model without any expression. ${\it generate}(m)$
  non-deterministically returns a grid parse tree that is obtained by
  replacing unknowns in the model by type-compatible sub-trees.
\end{definition}

\paragraph{Version 2.} We simply replace unknowns by default
values. It makes it very unlikely to generate the right output grid by
chance but it helps to visualize the intermediate states of the
learning process. The default position is $(0,0)$, the default size is
$(10,10)$ for grids, and $(2,2)$ for rectangles, the default color is
black, the default mask is the full mask, the default shape is a
rectangle with default values in each field.


\subsection{Reading and Writing a Grid with a Grid Model}
\label{read:write}

We define two important components for reading and writing grids.
They are simply composed from the above primitve functions, and are
used as basic blocks for training and using models.

\begin{figure}[t]
  \centering
  \begin{minipage}[b]{0.55\textwidth}
    \centering
    \includegraphics[height=3.2cm]{fig/read.pdf}
    \caption{The {\em read} component.}
    \label{fig:read}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.44\textwidth}
    \centering
    \includegraphics[height=3.2cm]{fig/write.pdf}
    \caption{The {\em write} component.}
    \label{fig:write}
  \end{minipage}
\end{figure}

\begin{definition}[grid reading]
  Let $m$ be a model. Function ${\it read}_m$ takes an environment and
  a grid as input, and returns a grid parse tree and delta as output.
  \[ {\it read}_m(\varepsilon,g) = \{ (\pi,\delta) \mid m_\varepsilon
    = {\it apply}(m,\varepsilon), \pi \in {\it
      parse}(m_\varepsilon,g), \delta = g - {\it draw}(\pi) \} \]
\end{definition}

Figure~\ref{fig:read} gives a schematic description of the {\em read}
component. When reading a grid~$g$, the grid model is first applied to
the input environment, in order to resolve any expression that the
model may contain. The resulting model is then used to parse the input
grid into a grid parse tree~$\pi$. Finally, the grid delta between the
grid specified by~$\pi$ and~$g$ is computed. The pair~$(\pi,\delta)$
provides a lossless representation of the input grid, as expressed by
the following lemma.
\[ (\pi,\delta) \in {\it read}_m(\varepsilon,g) \Longrightarrow g =
  {\it draw}(\pi) + \delta. \] %% TODO: prove it as a lemma


\begin{definition}[grid writing]
  Let $m$ be a grid model. Function ${\it write}_m(\varepsilon)$ takes
  an environment as input, and returns a grid as output.
  \[ {\it write}_m(\varepsilon) = \{ g \mid m_\varepsilon = {\it
      apply}(m,\varepsilon), \pi = {\it generate}(m_\varepsilon), g =
    {\it draw}(\pi) \} \]
\end{definition}

Figure~\ref{fig:write} gives a schematic description of the {\em
  write} component. When writing a grid, the grid model is first
applied to the input environment, in order to resolve any expression
that the model may contain. The resulting model is then used to
generate a grid parse tree, by instantiating the unknowns, from which
a concrete grid can be drawn.


\subsection{Task Models: Prediction and Training}
\label{training:predicting}

% TODO: schema showing the information flow
% involving: gi, mi, pi, di, mo, po, do

\begin{figure}[t]
  \centering
  \includegraphics[width=0.6\textwidth]{fig/predict.pdf}
  \caption{Prediction mode: producing an output grid from the input grid}
  \label{fig:predict}
\end{figure}

Using a task model~$M = {\bf InOut}(m^i,m^o)$ to predict the output
grid from the input grid simply consists into the chaining of grid reading and grid writing.

% the input grid model, and writing the resulting input grid parse tree into an output grid.

\[ {\it predict}_M(g^i) = \{g^o \mid (\pi^i,\delta^i) \in {\it read}_{m^i}({\it nil},g^i), g^o \in {\it write}_{m^o}(\pi^i) \} \]

Figure~\ref{fig:predict} gives a schematic description of
prediction. The input grid~$g^i$ is first read with the input grid
model~$m^i$, using an empty environment, which non-deterministically
results in an input grid parse tree~$\pi^i$ and an input
delta~$\delta^i$. Then, the predicted output grid is produced by
writing the output grid model~$m^o$ using the input grid parse tree as
environment.

For instance, given the input grid parse tree returned by the reading
of the test input grid in Figure~\ref{fig:task}, the output grid model
generates a 2x2 green rectangle at coordinates (2,2), on top of a 6x6
light blue background, which is the expected grid.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.6\textwidth]{fig/train.pdf}
  \caption{Training mode: reading the input and output grids in chain}
  \label{fig:train}
\end{figure}

\paragraph{}
During the training phase, both the input and output grids are
known. However, computing a grid delta between the predicted output
grid and the expected output grid is not enough to learn a better
model. It is more useuful to have the grid parse tree for each grid,
because it provides an explanation of how the model ``sees'' the
grids. We therefore define the ${\it train}$ function that chains the
reading of the input and ouput grids.

\HIDE{ On output grids~$y_k$, we use parsing rather than
  generating. This is explained by the fact that description lengths
  are used at the learning stage rather than at the application
  stage. During learning, output models are incomplete, and still
  require some parameters and delta to generate the output
  grid~$y_k$. However, this output grid is available during learning
  (for train examples), and using parsing enables to find which
  parameters and delta are required. The objective of learning is to
  make those output parameters and delta become empty. Knowing what
  are the required parameters and delta can help the learner improve
  the model in this direction.}


\[\begin{array}{rl}
    {\it train}_M(g^i,g^o) = \{ (\pi^i,\delta^i,\pi^o,\delta^o) \mid & (\pi^i,\delta^i) \in {\it read}_{m^i}({\it nil},g^i),\\
                                                                     & (\pi^o,\delta^o) \in {\it read}_{m^o}(\pi^i,g^o) \}
  \end{array}\]

Figure~\ref{fig:train} gives a schematic description of training. The
first step is the same as prediction. The input~$g^i$ is first read
with the input grid model~$m^i$, using an empty environment, which
non-deterministically resuts in an input grid parse tree~$\pi^i$ and
an input grid delta~$\delta^i$. In the second step, the output
grid~$g^o$ is read with the output grid model~$m^o$, using $\pi^i$ as
environment, which non-deterministically results in an output grid
parse tree~$\pi^o$ and an output grid delta~$\delta^o$.
%
The two grid parse trees and the two grid deltas collectively form a
lossless description of the two grids according to the task model.
 

%%%
\section{Learning Task Models}
\label{learning}

MDL-based learning works by selecting the model that compresses the
data the best. This involves two main components: (a) a measure of
compression so as to compare models, and (b) a strategy to explore the
model space. The measure of compression is called {\em description
  length}.

\subsection{Description Lengths}
\label{dl}

In two-part MDL, the description length (DL)~$L(M,D)$ measures the number
of bits required to communicate data~$D$ with model~$M$. It is
decomposed in two parts according to the equation
\[ L(M,D) = L(M) + L(D|M) \] where $L(M)$ is the DL of the model, and
$L(D|M)$ is the DL of the data encoded with the model.
%
In our case, a model~$M$ is a task model~${\bf InOut}(m^i,m^o)$, and
data~$D$ is the part of a task~$T = (E,F)$ that is accessible to
the learner. As the objective is to learn a model that generalizes to
new instances, only the train examples are accessible, so
$D = E$.
% \footnote{In Version~1, test input grids were wrongly considered as part of~$T^-$. In the following, we only use $T^-$ relative to Version~1.}.

The description length of the task model is the sum of the description
lengths of its two grid models.
\[ L(M) = L(m^i) + L(m^o) \]
%
We detail the description length of grid models below. The
description length of the task data~$D = E$ is the sum of the
description length of each example.
\[ L(D|M) = L(E|M) = \alpha \sum_{(g^i,g^o) \in E} L(g^i,g^o|M) \]
%
Factor~$\alpha$ equals~10 by default to give more weights to the data,
relative to the model, and hence allow the learning of more complex
models. Indeed, ARC tasks have a very low number of examples (3 on
average).

The description length of a pair of grids, according to a task model,
is based on the chained reading of the two grids. It boils down to
description lengths of grid parse trees and grid deltas. As reading
grids is non-deterministic, we define the description length as the
minimum DL over all readings, thus choosing the ``best'' reading.
\[\begin{array}{ll}
    L(g^i,g^o|M) = {\it min}\ \{ & L(\pi^i|m^i) + L(\delta^i|\pi^i) + L(\pi^o|{\it apply}(m^o,\pi^i)) + L(\delta^o|\pi^o)\\
                             & \mid (\pi^i,\delta^i,\pi^o,\delta^o) \in {\it train}_M(g^i,g^o)\ \}
  \end{array}\]

Note that the description lengths~$L(\pi|m)$ of grid parse trees~$\pi$
are relative to the grid model~$m$ (if necessary, applied to its
environment). Indeed, we only need to encode the parts of the grid
parse tree that are unknown in the grid model or that differ from the
grid model. Its detailed definition depends on the definition of grid
models.

The description length~$L(\delta|\pi)$ of a grid delta~$\delta$ is
relative to a grid parse tree~$\pi$. This is valid as the grid parse
tree is encoded before the delta, and is therefore available when
encoding the grid delta. Its detailed definition also slightly depends
on grid models.

When defining description lengths for grid models and grid parse
trees, we assume a few basic description lengths for elementary types:
\begin{itemize}
\item $L_\nat(n) = 2\,\log (n+1) + 1$: universal encoding of natural
  intergers (including zero), more precisely the gamma encoding
  shifted by one to include zero;
\item $L_P(x \in X) = -\log P(x)$: encoding based on a probability distribution~$P$ over elements~$X$;
\item $L_X(x \in X) = \log |X|$: encoding based on a uniform distribution over elements~$X$.
%\item $L_C(c) = \log |C|$: encoding of colors along a uniform distribution.
\end{itemize}


\paragraph{Version~2.}
In order to define the description lengths of grid models and grid
parse trees, we first need to define the description lengths of
primitive types, algebraic datatypes, expressions, and templates.

{\em Primitive types.} Integers are used for positions and sizes. For
positions, we use a uniform distribution over the rows or columns of a
grid. Hence for a row position~$i$ in a grid~$g$,
$L(i|g) = \log {\it height}(g)$; similarly, for a column position~$j$
in a grid~$g$, $L(j|g) = \log {\it width}(g)$. When the grid size is
not known, the maximum grid size is used (30 in the ARC challenge).
%
For sizes, we use the universal encoding of natural numbers~$L_\nat$.
%
For the color of shapes, we use the uniform distribution~$L_C$. For
the background color of grids, we use a probability
distribution~$P_{BC}$ that gives a higher probability to color black
($P_{BC} = 0.91$) than to other colors ($P_{BC} = 0.01$). Indeed, in
most tasks, grids have a black background. Hence, for a background
color~$c$, $L(c) = -\log P_{BC}(c)$. \KILL{We then define the
  description length of background colors $L_{BC}(c)$ as
  $L_{P_{bc}}(c)$.}
%
As the mask of a rectangle is a bitmap, we simply define its
description length as its number of bits, i.e. its height times its
width, which are known from the rectangle size. However, as it is
common to have full masks for pure rectangles, we use a bit to encode
whether the mask is full or not, and we then encode the bitmap only if
it is not full.
\[ L(mask|h,w) = \left\{
  \begin{array}{l@{\quad}l}
    1 & \textrm{when the mask bitmap is full} \\
    1 + hw & \textrm{otherwise} \\
  \end{array}\right. \]
When the rectangle size is not known, e.g. in a model with unknowns, a
constant bitmap cannot be specified and the only possible value is the
full mask.

{\em Algebraic types.} For each algebraic datatype (${\it Grid}$,
${\it Shape}$, ${\it Vector}$), each constructor determines which
fields are to be encoded. Therefore, it suffices to encode the choice
of a constructor among the constructors of the datatype, in addition
to the encoding of the fields. For datatypes~${\it Grid}$ and
${\it Vector}$, there is a single constructor, hence there is no
choice to be made. Hence, $L({\bf Grid}) = L({\bf Vec}) = 0$. For
datatype~${\it Shape}$, there are two constructors, to which we give a
uniform distribution. Hence,
$L({\bf Point}) = L({\bf Rectangle}) = -\log 0.5 = 1$.
%
For a given constructor, the encoding of some field may depend on
other fields, thus inducing an ordering of fields. This is the case
for constructor~${\bf Rectangle}$ where the mask field depends on the
size field (see the encoding of masks above).\footnote{We could also consider
that the position field depends on the size field. Indeed, given a
grid size and a rectangle size, not all positions are possible if we
assume that the rectangle must fit into the grid.}
%\[ L({\bf Rectangle}(pos,size,c,m|g) = L({\bf Rectangle}) + L(pos|g) + L(size|g) + L(c) + L(m|size) \]
%
Grids have a list of shapes. The encoding of a list starts with the
universal encoding of the length of the list, followed by the encoding
of the list elements.

{\em Expressions.} There are three kinds of expressions (with
distribution~$P_e$): primitive values and constructors ($P_e = 0.25$,
2 bits), function applications ($P_e = 0.25$, 2 bits), and variables
($P_e = 0.5$, 1 bit). Primitive values and constructors are encoded as
described above. Functions are encoded along a uniform distribution
over the functions whose result type is the expected type. The
expected type is determined by the syntactic context of the
expression: e.g., a color if it is the color field of the grid or a
shape, an integer if it is a component of the position of a shape, or
an operand of an arithmetic operator. Currently, there are only two
functions, addition and substraction, both with result type
${\it int}$. Variables are encoded according to the environment
signature, i.e. the set of variables that are available in the
environment, grouped by data type. A variable is encoded according to
a probability distribution over the environment variables of same
type. In order to distinguish between the same-type variables, we
define the probability distribution as the softmax of the similarities
between the paths of those variables with the expression path. This
implies that defining a shape height as a function of a shape height
is prefered to defining it as a function of a shape width or a shape
position.

{\em Templates.} There are three kinds of templates (with
distribution~$P_t$): primitive value and constructors ($P_t = 0.4$,
1.3 bits), expressions ($P_t = 0.5$, 1 bit), and unknowns
($P_t = 0.1$, 3.3 bits). In a template, each field of a constructor is
a template, thus allowing unknowns and expressions at any place. In
practice, we never use unknowns for whole grid models, and for whole
shapes. Expressions are given the higher probability because they
explain some part of the grid parse tree from the
environment. unknowns are given the lower probability because they
correspond to unexplained parts.

{\em Grid models} ($L(m)$) are templates over type ${\it Grid}$,
and are encoded as such.

{\em Grid parse trees} ($L(\pi|m)$) are only made of primitive values
and constructors, and could be encoded as such. However, as grid parse
trees are obtained by parsing grids according to a grid model, parts
of the grid parse tree are already encoded in the grid
model. Actually, the very purpose of grid models is precisely to
factor out what all example grids have in common, and avoiding to
encode it repeatedly. The only parts of grid parse trees that need to
be encoded are (a) the parts corresponding to unknowns in the grid
model, and (b) the parts that differ from the grid model due to
approximate parsing.
\begin{itemize}
\item[(a)] About the unknowns, a fixed ordering can be derived from
  the model. Therefore, it is enough to concatenate the encodings of
  the grid parse subtrees that correspond to each unknown.
\item[(b)] About the differing parts, we have no apriori information,
  neither on their number, nor on their location in the grid parse
  tree. We first encode their number with universal encoding. We then
  encode each differing part by encoding its location in the grid
  parse tree, using a uniform distribution, and the subtree at that
  location.
\end{itemize}
The encoding of the location of the differing parts could probably be
improved in several ways. First, some differences are more likely than
others: e.g., a different rectangle width vs a completely different
shape. Second, not all subsets of locations make sense as a difference
location should not be inside another difference location as it would
entail to encode twice the same part.

{\em Grid delta} ($L(\delta|\pi)$). For comparability with grid models and grid parse
trees, we encode a grid delta~$\delta$ as a set of points.  The
description length of a grid delta is hence defined as:
\[ L(\delta|\pi) = L_\nat(|\delta|) + \sum_{(i,j,c) \in \delta} L({\bf Point}({\bf Vec}(i,j),c)|{\it draw}(\pi)). \]
%
Note that the description length of points is made relative to the
grid drawn from the grid parse tree. In particular, this makes the
grid size available to the encoding of the point position.

\KILL{
\paragraph{Version~1 } uses the following definition.
\[ L(h,w,\delta) = L_\nat(|\delta|) + log(C_{hw}^{|\delta|}) + |\delta|log(|C|-1) \]
%
The first term encodes the number of cells in the delta. The second
term encodes which cells in the grid ($hw$ cells in total) are in the
delta, using a combination. The third term encodes the colors of those
delta cells, choosing for each among the $|C|-1$ other colors, other
than the color generated with the model and parameters.

\paragraph{Version~1.1} introduces a more naive encoding to make it
more comparable to models made of point shapes.
\[ L(h,w,\delta) = |\delta|(log\ h + log\ w + log\ |C|) \]

\paragraph{}
We define the code length of a bitmap~$m \in M$ as follows,
assuming that the grid dimensions $h \times w$ are known from the
shape, and given that there are $k$ activated pixels in the bitmap.
\[ L(m|h,w) = \left\{
  \begin{array}{l@{\quad}l}
    1 & \textrm{when the bitmap is full} \\
    1 + hw & \textrm{otherwise} \\
  \end{array}\right. \]

%%% V1

\begin{table}[t]
\caption{Definition of the description length of models and their parameters}
\begin{center}
\begin{datatype}
  {\it task} \is {\bf InOut}({\it grid}_1, {\it grid}_2)
  \baction
  L(task) \defby L(grid_1,0) + L(grid_2,|U(grid_1)|)
  \eaction
  \\
  {\it grid} \is {\bf Background}(height: {\it attr}_\nat, width: {\it attr}_\nat, color: {\it expr}_C)
  \baction
  L(grid,N_V) \defby 1 + L(height,N_V,L_\nat) + L(width,N_V,L_\nat) + L(color,N_V,L_C)
  \eaction
  \altis {\bf AddShape}({\it shape}, {\it grid}_1)
  \baction
  L(grid,N_V) \defby 1 + L(shape,N_V) + L(grid_1,N_V)
  \eaction
  \\
  {\it shape} \is {\bf Rectangle}(height: {\it attr}_\nat, width: {\it attr}_\nat,\\
  & & \hspace*{1cm} mini: {\it attr}_\nat, minj: {\it attr}_\nat, color: {\it attr}_C)
  \baction
  L(shape,N_V) \defby L(height,N_V,L_\nat) + L(width,N_V,L_\nat) \\
  & + & L(mini,N_V,L_\nat) + L(minj,N_V,L_\nat) + L(color,N_V,L_C)
  \eaction
  \\
  {\it attr}_\alpha \is {\bf Unkown}(ident)
  \baction
  L(attr,N_V,L_\alpha) \defby 1 \\
  L_{m,ident}(c) \defby L_\alpha(c)
  \eaction
  \altis {\bf Expr}({\it expr}_\alpha)
  \baction
  L(attr,N_V,L_\alpha) \defby 1 + L(expr,N_V,L_\alpha)
  \eaction
  \\
  {\it expr}_\alpha \is {\bf Var}(ident)
  \baction
  L(expr,N_V,L_\alpha) \defby -log\ 0.3 + log\ N_V
  \eaction
  \altis {\bf Const}(c: \alpha)
  \baction
  L(expr,N_V,L_\alpha) \defby -log\ 0.5 + L_\alpha(c)
  \eaction
  \altis {\bf Plus}({\it expr}_1, {\it expr}_1)
  \baction
  L(expr,N_V,L_\alpha) \defby -log\ 0.1 + L(expr_1,N_V,L_\alpha) + L(expr_2,N_V,L_\alpha)
  \eaction
  \altis {\bf Minus}({\it expr}_1, {\it expr}_2)
  \baction
  L(expr,N_V,L_\alpha) \defby -log\ 0.1 + L(expr_1,N_V,L_\alpha) + L(expr_2,N_V,L_\alpha)
  \eaction
\end{datatype}
\end{center}
\label{tab:l:v1}
\end{table}

\paragraph{Version 1.} Table~\ref{tab:l:v1} defines the description
length (DL) of models (version~1), and of their
parameters. Function~$L$ is defined for all types, and for each type
variant. Depending on each type, it may take additional
parameters. The DL of a model is essentially the sum of the DLs of its
components. When there are several variants in a type, bits are added
to encode which variant applies. For types $grid$ and $attr$, 1 bit is
used as there are two variants; for type~$expr$, the number of bits is
derived from a probability distribution, giving more weight to
constants and variables than to addition and substraction.

The DL of a grid model depends on the number~$N_V$ of variables in its
environment, 0 for the input grid model, and the number of input
unknowns for the output grid model. This number~$N_V$ is passed down
to expression variables, which use it to encode a variable according to a
uniform distribution among the $N_V$ environment variables.

The DL of attributes and expressions, as they are generic in the type
of values, take as parameter a function~$L_\alpha$ for the encoding of
constants of that type (typically $L_\nat$ for integers, and $L_C$ for
colors).

Unknowns are encoded with a single bit in the model, to encode the
choice between an unknown and an expression. The encoding of their
values is delayed to the encoding of the grid model parameters~$\pi$,
as part of ``data according to the model''.  Function~$L_{m,ident}$
defines the DL of that delayed encoding for the unknown named
$ident$. It is here simply defined as the constant DL~$L_\alpha$ but
refinements are possible based on the knowledge of the grid model as
prior. For instance, uniform encoding can be used instead of universal
encoding for integers when bounds are known for the attribute value
(e.g., grid size).
}%KILL

\begin{table}[t]
  \centering
  \caption{Comparison of description lengths of the initial model (left) and the solution model given above (right).}
  \begin{minipage}{0.4\textwidth}
    \begin{center}
      \begin{tabular}{|l||r|r||r|}
    \hline
          & $L(M)$ & $L(D|M)$ & $L(M,D)$ \\
    \hline
    \hline
    input & 9.0 & 9984.0 & 9993.0 \\
    \hline
    output & 9.0 & 1634.7 & 1643.7 \\
    \hline
    \hline
    chained & 18.0 & 11618.7 & 11636.7 \\
    \hline
      \end{tabular}
    \end{center}
  \end{minipage}
\hfill
\begin{minipage}{0.4\textwidth}
  \begin{center}
    \begin{tabular}{|l||r|r||r|}
    \hline
      & $L(M)$ & $L(D|M)$ & $L(M,D)$ \\
    \hline
    \hline
    input & 68.9 & 1711.3 & 1780.2 \\
    \hline
    output & 50.6 & 0.0 & 50.6 \\
    \hline
    \hline
    chained & 119.5 & 1711.3 & 1830.8 \\
    \hline
    \end{tabular}
  \end{center}
\end{minipage}
\label{tab:dl:example}
\end{table}

\paragraph{}
For the task in Figure~\ref{fig:task}, Table~\ref{tab:dl:example}
compares the description lengths of the initial model and the solution
model given as example above, for $\alpha=10$ (each example counts as
10). The tables detail the split between input grid and output grid on
one hand, and the split between the model ($L(M)$) and the data
according to the model ($L(D|M)$) on the other hand.
%
We observe that the increase in the model DL is largely compensated by
the decrease of the data DL. The resulting compression gain is 16.3\%, a six-fold
reduction. In particular, with the solution model, the description
length of the ouput grids has fallen to zero, which means that output
grids are entirely explained by the model and the input grid parse
tree.

\KILL{
\paragraph{Version 1.1.} The description length for shapes is extended
in the obvious way, giving the same weight to points and rectangles.

\begin{datatype}
  {\it shape} \is {\bf Point}(i: {\it attr}_\nat, j: {\it attr}_\nat, color: {\it attr}_C)
  \baction
  L(shape,N_V) \defby 1 + L(i,N_V,L_\nat) + L(j,N_V,L_\nat) + L(color,N_V,L_C) \\
  \eaction
  \altis {\bf Rectangle}(height: {\it attr}_\nat, width: {\it attr}_\nat, mini: {\it attr}_\nat, minj: {\it attr}_\nat,\\
  & & \hspace*{1cm} color: {\it attr}_C, mask: {\it attr}_M)
  \baction
  L(shape,N_V) \defby 1 + L(height,N_V,L_\nat) + L(width,N_V,L_\nat) \\
  & + & L(mini,N_V,L_\nat) + L(minj,N_V,L_\nat) + L(color,N_V,L_C) + L(mask,N_V,L_M)
  \eaction
\end{datatype}

\begin{center}
  \begin{tabular}{|l|r|r|r|}
    \hline
    model & $L(m)$ & $L(E|m)$ & $L(m,E)$ \\
    \hline
    initial & 8.0 & 23315.5 & 23323.5 \\
    example & 114.1 & 1788.4 & 1902.6 \\
    \hline
  \end{tabular}
\end{center}
}%KILL


\subsection{Learning Strategy}

The model space is generally too vast for a systematic exploration,
and heuristics have to be employed to guide the search for a good
model.
%
The learning strategy consists in the iterative refinement of models,
starting with an initial model, in search for the best model, i.e. the
most compressive model. Given a class of models, a search strategy is
therefore entirely specified by:
\begin{enumerate}
\item an initial model~$M_o$ ({\em initial model}), generally the
  simplest one;
\item and a function~$R$ mapping each model~$M$ to an ordered
  list~$M_1, M_2, \ldots$ of refined models ({\em refinements}),
  generally taking into account the data according to the model (here,
  grids, grid parse trees and grid deltas).
\end{enumerate}
We say that a refinement~$M_i$ is {\em compressive} if it reduces the
description length compared to the original model~$M$. The ordering of
refinements is important because the number of possible refinements
could be huge, and it has a cost to evaluate each refinement by
measuring its description length. Indeed, this involves the parsing of
both input and output grids by each refined model.  This ordering is
therefore a key part of the heuristics.

Another part of the heuristics consists in bounding the number of
compressive refinements per model that are considered, and the number
of models that are kept after each iteration. A {\em greedy search}
selects the first compressive refinement, and has therefore a single
model at each iteration. A {\em beam search} provides a wider
exploration by selecting $K_r$ compressive refinements per model, and
then selecting among them the $K_m$ best models for the next
iteration. $K_m$ is the beam width. The search halts when no
compressive refinement can be found.
% TODO: think again about strategy, look at beam stack search for instance, anytime+backtracking... complete if enough time

\paragraph{Version 2.} The initial model is made of two minimal grid
models, reduced to a background with unknown size and color, and no
shape layer.
\[ {\bf InOut}({\bf Grid}(?,?,[\ ]), {\bf Grid}(?,?,[\ ])) \]

Two kinds of refinements are used: (1) the addition of a shape to a
grid model (input or output), and (2) the replacement of a part of a
template (typically some unknown) by another template (typically a
value or an expression).
%
When adding a shape, its kind -- point or rectangle -- is fixed but
all its fields are left unspecified. The added shape template is
therefore either ${\bf Point}(?,?)$ or ${\bf Rectangle}(?,?,?,?)$.  If
there is already a non-empty list of shapes, the new shape can be
inserted at any position in the list, from bottom to top.

The second kind of refinements enables to replace part of a model at
path~$p$ by a template~$t$. The replacing template is either a {\em
  pattern}, i.e. a combination of primitive values, constructors, and
unknowns; or an {\em expression} using environment variables. For the
input grid model, it can only be patterns because the environment is
empty. For the output grid model, every path into the input grid parse
tree is available as an environment variable.
%
In this version, we only consider patterns made of a single primitive
value or a single constructor with all fields being unknowns; and we
only consider expressions on integers with a form among $x$, $x+c$,
$x-c$, $x+y$, $x-y$, for any environment variables~$x,y$, and any
constant integer~$c$.
%
When the replaced part~$p$ is an unknown, it enables to make the model
more specific to the task. When the replaced part is a pattern in the
output grid model, and it is replaced by an expression, it enables to
better define the output grid as a function of the input grid. To
summarize, unknowns may be replaced by patterns and expressions, and
patterns in the output grid model may be replaced by expressions.

Let $E$ be the set of examples (pairs of grids), and $M = (m^i,m^o)$
be the current model. We generate the following refinements:
\begin{enumerate}
\item An input unknown at~$p \in U(m^i)$ can be replaced by
  pattern~$t$ if
\[ \forall (g^i_k,g^o_k) \in E: \exists (\pi^i_k,\delta^i_k) \in {\it
    read}_{m^i}(nil,g^i_k): \pi^i_k.p \sim t \]
%
\item An output unknown at~$p \in U(m^o)$ can be replaced by
pattern~$t$ if
\[ \forall (g^i_k,g^o_k) \in E: \exists (\pi^i_k,\delta^i_k,\pi^o_k,\delta^o_k) \in {\it
    train}_M(g^i_k,g^o_k): \pi^o_k.p \sim t \]
%
\item An output grid model part at~$p \in m^o$ can be replaced by expression~$e$ if
\[ \forall (g^i_k,g^o_k) \in E: \exists (\pi^i_k,\delta^i_k,\pi^o_k,\delta^o_k) \in {\it
    train}_M(g^i_k,g^o_k): \pi^o_k.p = {\it apply}(e, \pi^i_k) \]
\end{enumerate}


A tentative ordering of refinements is the following: output shapes,
input shapes, output expressions, input expressions.  Among
expressions, the ordering is: $x$, $x-c$, $x+c$, $x-y$, $x+y$.

The sequence of compressive refinements that leads to the above model
about task in Figure~\ref{fig:task} is shown in the table below. The
description length includes both the model and the data, and both the
input grid and the output grid. It is the description length reached
after applying the refinement. Each refinement is described as an
equality between a model path (left-hand side) and a template
(right-hand side). Refinements defining a layer by a shape actually
inserts a new layer for that shape.

\begin{center}
  \begin{tabular}{|r|r|l|}
    \hline
    step & $L(M,D)$ & refinement \\
    \hline
    1 & 4461.6 & $in.layer[1] = {\bf Rectangle}(?,?,?,?)$ \\
    2 & 4047.2 & $out.size = layer[1].size$ \\
    3 & 3650.4 & $in.layer[0] = {\bf Rectangle}(?,?,?,?)$ \\
    4 & 3352.6 & $out.layer[0] = {\bf Rectangle}(?,?,?,?)$ \\
    5 & 3114.6 & $out.color = in.layer[0].color$ \\
    6 & 2896.4 & $out.layer[0].size = in.layer[0].size$ \\
    7 & 2757.0 & $out.layer[0].color = in.layer[1].color$ \\
    8 & 2686.3 & $out.layer[0].mask = full$ \\
    9 & 2615.7 & $in.layer[0].mask = full$ \\
    10 & 2545.0 & $in.layer[1].mask = full$ \\
    11 & 2499.4 & $in.color = black$ \\
    12 & 2464.4	& $out.layer[0].pos = {\bf Vec}(?,?)$ \\
    13 & 2374.4	& $out.layer[0].pos.i = in.layer[0].pos.i - in.layer[1].pos.i$ \\
    14 & 2284.5	& $out.layer[0].pos.j = in.layer[0].pos.j - in.layer[1].pos.j$ \\
    15 & 2249.5	& $in.size = {\bf Vec}(?,?)$ \\
    16 & 1970.9	& $in.size.i=12$ \\
    17 & 1935.9	& $in.layer[0].pos = {\bf Vec}(?,?)$ \\
    18 & 1900.9	& $in.layer[0].size = {\bf Vec}(?,?)$ \\
    19 & 1865.9	& $in.layer[1].pos = {\bf Vec}(?,?)$ \\
    20 & 1830.8	& $in.layer[1].size = {\bf Vec}(?,?)$ \\
    \hline
  \end{tabular}
\end{center}

At steps 1 and 3, two rectangle shapes are introduced in the input.
At step 4, a rectangle shape is introduced in the output. Combined
with the background grid, this is enough to cover all cells in both
grids. As of step 4, there are therefore empty grid deltas.
%
At steps 2 and 5, the size and color of the output grid are
respectively equated to the bottom input shape ($in.layer[1].size$)
and to the top input shape ($in.layer[0].color$). At steps 6 and 7,
the size and color of the output shape are equated to respectively to
the top input shape ($in.layer[0].size$), and to the bottom input
shape ($in.layer[1].color$). At this stage, only the position of the
output shape remains to be specified.
%
Steps 8-10 finds that all shapes are {\em full} rectangles. Step 11
finds that the input background color is black.
%
Steps 12-14 expand the output shape position as a vector, and find the
expressions that define those positions, here differences between the
top input shape and the bottom input shape.
%
At this stage, the model has solved the task as it can correctly
generate the output grid for any input grid.
%
The other steps expand the remaining positions and sizes as vectors,
and find that all input grids have 12 rows. The latter is an
accidental regularity, and approximate parsing is necessary to parse
the test instance as it has 14 rows.


%%%
\section{Evaluation}
\label{eval}


\begin{table}[t]
  \centering
  \caption{Performance on training and evaluation tasks as the number of tasks solved, per version}
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
     & & & & & \multicolumn{2}{c|}{training} & \multicolumn{2}{c|}{evaluation} \\
    version & date & $\alpha$ & timeout & avg. time & train & test & train & test \\
    \hline
    v1 & & ~10~ & 20s & 9s & ~9 / 9.6~ & ~5 / 5.5~ & ~0 / 0.3~ & ~0 / 0.0~ \\
    \hline
    v1.1 & 2021-03-01 & ~10~ & 2s & 1.53s & ~16 / 17.3~ & ~5 / 5.0~ & ~4 / 4.3~ & ~4 / 4.0~ \\
    \hline
%    v2 & 2021-03-31 & ~10~ & 10s & 7.6s & ~14 / 19.2~ & ~14 / 14.0~ & ~2 / 3.6~ & ~2 / 2.0~ \\
%    v2 & 2021-03-31 & ~10~ & 30s & 19.0s & ~16 / 25.3~ & ~18 / 18.5~ & ~2 / 4.3~ & ~3 / 3.0~ \\
%    v2 & 2021-04-26 & ~10~ & 30s & 24.1s & ~23 / 27.0~ & ~17 / 17.0~ & ~3 / 4.9~ & ~3 / 3.0~ \\
%    v2 & 2021-04-26 & ~10~ & 60s & 35.4s & ~36 / 40.8~ & ~25 / 25.0~ &  ~6 / 8.6~ & ~6 / 6.0~ \\
    v2 & 2021-05-01 & ~10~ & 30s & 20.3 & ~28 / 34.6~ & ~19 / 19.5~ &   &  \\
    v2 & 2021-05-01 & ~10~ & 60s & 35.1s & ~33 / 41.2~ & ~22 / 22.5~ &  ~7 / 9.9~ & ~6 / 6.0~ \\
    v2 & 2021-05-01 & ~10~ & 120s &  & & &  ~8 / 10.9~ & ~7 / 7.0~ \\
    \hline
  \end{tabular}
  \label{tab:eval}
\end{table}

Table~\ref{tab:eval} reports on the performance of our approach on
both training and evaluation tasks (400 tasks each), for successive
versions and timeouts. The timeout sets a limit to the learning time
for each task. On both collections of tasks, we provide performance
for train examples, for which output grids are known, and for test
examples. In each case, we give two measures $n_1/n_2$: $n_1$ is the
number of tasks for which {\em all} examples are correctly solved, and
$n_2$ is the sum of partial scores over tasks. For instance, if a task
has two test examples and only one is solved, the score is~$0.5$.

\paragraph{Version 1.} This first version was designed by looking at
tasks {\tt ba97ae07} and {\tt b94a9452}. Evaluating it on training
tasks has shown that this simple model manages to solve 3.5 additional
tasks: {\tt 6f8cd79b}, {\tt 25ff71a9} (2 correct examples), {\tt
  5582e5ca}, {\tt bda2d7a6} (1 correct example out of 2). This version
even manages to explain all train examples of 3 additional tasks,
hence 8 tasks in total, although they fail on the related test
examples: tasks {\tt a79310a0} (3 examples), {\tt bb43febb} (2
examples), {\tt 694f12f3} (2 examples), and {\tt a61f2674} (2
examples). For task {\tt a79310a0}, v1 recognizes that a shape is
moved, and changes color but, whereas all train shapes are rectangles,
the test shape is not a rectangle. For tasks {\tt bb43febb} and {\tt
  694f12f3}, v1 finds unplausible expressions to replace some
unknowns, e.g. replacing some height by a position instead of a height
decremented by a constant, or replacing some unknown by a constant
that happens to be valid for train examples but not for the test
example. For task {\tt a61f2674}, v1 is not powerful enough but given
that there are only two train examples, it manages to find some ad-hoc
regularity.

Despite those encouraging result on training tasks, no evaluation task
could be solved with this version. We wonder whether this is a
statistical fluke or the fact that evaluation tasks are harder than
the training tasks.

%% training v1.1
\begin{table}[t]
  \centering
  \caption{Performance on training tasks as the number of tasks solved, for different strategies, in Version~1.1 with timeout 2s, and $\alpha=10$. C = constants first, V = variables first, Ei = input expressions, Si = input shapes, Eo = output expressions, So = output shapes.}
  \begin{tabular}{|c|c|c|c|}
    \hline
    strategy & train & test \\
    \hline
    V/Si-Ei-So-Eo & ~11 / 11.7~ & ~4 / 4.0~ \\
    V/Ei-Si-So-Eo & ~14 / 15.0~ & ~5 / 5.0~ \\
    V/Ei-Si-Eo-So & ~16 / 17.3~ & ~5 / 5.0~ \\
    C/Ei-Si-Eo-So & ~16 / 17.3~ & ~5 / 5.0~ \\
    \hline
  \end{tabular}
  \label{tab:v11:strategies}
\end{table}

\paragraph{Version 1.1.}  Table~\ref{tab:v11:strategies} compare
results with Version~1.1 for different strategies concerning model
refinements. For example, the first line consider expressions with
variables before constants, and consider refinements in this order:
input shapes, input expressions, output shapes, output expressions.
The best strategy among those tested is Ei-Si-Eo-So. Refining the
input model first provides useful parameters when refining the output
model. Inserting expressions before inserting new shapes helps to
disambiguate the parsing process, which often happens when a model
contains several unspecific shapes.

The successful training tasks are {\tt ba97ae07}, {\tt bda2d7a6}, {\tt
  6f8cd79b}, {\tt e48d4e1a}, {\tt 25ff71a9}. Compared to Version~1, we
improve on {\tt bda2d7a6} by succeeding on the the two test instances,
and gain {\tt e48d4e1a}. The former is explained by the use of masks,
and the latter is explained by the improved extraction of rectangles
from grids.
%
However, we loose on {\tt b94a9452} and {\tt
  5582e5ca}. In Version~1, the latter was successful by chance, and
the former because test inputs were considered for training.
%
Finally, 17.3 training instances are successful compared to 9.6 in the
previous version. Hence, all in all, we consider this new version as a
valuable improvement.

This is confirmed by Table~\ref{tab:eval} that shows that this new
version succeeds in 4 evaluation tasks, instead of none for the
previous version.

%% TODO: give parameter values
\paragraph{Version 2.} Table~\ref{tab:eval} reports results for
Version~2 for different timeouts. It uses a greedy search ($K_m = 1$)
but at each step, it look for up to 20 compressive refinements
($K_r = 20$), selecting the best one among them. Grid parsing looks
for up to 512 grid parse trees, and keep only the three best
ones. Approximate parsing is not activated on train examples, and
limited to 3 differences on test examples.
%
Table~\ref{tab:eval} shows a sharp improvement w.r.t. previous
versions, especially on the training tasks. The number of successful
tasks jumps from~5 to 22 in the training dataset, and from~4 to~7 in
the evaluation dataset. This seems to confirm that evaluation tasks
are intrisically more complex than training tasks.
%
As the language of models is the same as in Version~1.1, the
improvement come mostly from the way those models are used and
modified:
\begin{itemize}
\item the use of paths in grid parse trees rather than variable names,
\item the introduction of non-determinism in grid parsing,
\item the ordering of candidate grid parse trees and refinements,
\item the ability to replace any part of a template, not only unknowns,
\item better definitions of description lengths.
\end{itemize}
Higher timeouts are required because of the non-determinism that
significantly increases computation complexity.

For a timeout of 60s, the 22 successful training tasks are: {\tt
  1bfc4729}, {\tt 1cf80156}, {\tt 1f85a75f}, {\tt 25ff71a9}, {\tt
  445eab21}, {\tt 48d8fb45}, {\tt 5521c0d9}, {\tt 5582e5ca}, {\tt
  681b3aeb}, {\tt 6f8cd79b}, {\tt a1570a43}, {\tt a79310a0}, {\tt
  a87f7484}, {\tt aabf363d}, {\tt b1948b0a}, {\tt b94a9452}, {\tt
  ba97ae07}, {\tt bda2d7a6}, {\tt bdad9b1f}, {\tt e48d4e1a}, {\tt
  e9afcf9a}, {\tt ea32f347}.  We also know that task~{\tt 23581191}
can be solved with a higher timeout of~120s.
%
Task~{\tt 48d8fb45} is successful although the learned model does not
exhibit a proper understanding of it. The system found a correlation
that was probably not intended by the creator of the task.
%
The models learned on other tasks are adequate, and are rather diverse
despite the simplicity of the model language. We informally describe a
few learned models to illustrate this diversity.
\begin{itemize}
\item {\tt a79310a0}. Find a cyan rectangle and move it one row
  down. Unexpectedly, the test instance has an irregular shape instead
  of a rectangle. Thanks to approximate parsing, the input grid parse
  tree contains a mask of the irregular shape, which is used to
  generate the output grid.
\item {\tt aabf363d}. Find an arbitrary shape and a colored point in
  the bottom left corner, and output the same shape except for the
  color, which is the same as the point.
\item {\tt b94a9452}. Find an arbitray shape on a pink background,
  simply change the background color to red.
\item {\tt ba97ae07}. Find two full rectangles, reverse the list of
  layers: bottom rectangle on top, top rectangle at bottom.
\item {\tt bdad9b1f}. Find two segments, a 2x1 cyan rectangle, and 1x2
  red rectangle. Extend each segment into a line spanning the whole
  grid, then add a yellow point at the crossing of the two lines.
\item {\tt e48d4e1a}. Find two colored lines, a 1x10 rectangle and a
  10x1 rectangle, on a 10x10 black grid. Find also a grey rectangle in
  the top right corner with width=1. Generate the same lines at
  different positions, according to the height~$H$ of the grey
  rectangle. The vertical line is moved $H$ columns left
  (substraction), and the horizontal line is moved $H$ rows down
  (addition). This involves arithmetic expressions for computing the
  new positions.
\item {\tt ea32f347}. Find three grey full rectangles, implictly
  ordered from the larger (top layer) to the smaller (bottom layer)
  according to the parsing ordering heuristics. Generate the same
  rectangles with different colors: the top one gets blue, the middle
  one gets yellow, and the bottom one gets red.
\item {\tt 1cf80156}. Crop an arbitrary colored shape on a black grid.
\item {\tt 1f85a75f}. Crop an arbitrary colored shape on a black grid
  that also contains many points of other colors at random positions
  (a kind of noise).
\item {\tt 6f8cd79b}. Starting with a black grid of any size, generate
  a cyan grid of same size, and add a black rectangle at position
  (1,1) whose size is two rows and two columns less than the grid. The
  effect is to add a cyan border to the input grid.
\item {\tt 445eab21}. Find two shapes, the larger one implicitly on
  top. Generate a 2x2 grid whose color is the same as the top shape.
\item {\tt 681b3aeb}. Find two shapes on a black background. Generate
  a 3x3 grid with the color of one shape, and add at position (0,0)
  the other shape. Actually, the two shapes complement each other and
  pave the 3x3 grid. The learned model works because the evaluation
  protocol allows for three predicted output grids, and there are only
  two possibilities as the top left cell must belong to one of the two
  shapes. Moreover, as the larger shape is tried first and it has a
  higher probability to occupy the top left cell, there is more than
  50\% chance being correct on first trial.
\item {\tt 5521c0d9}. Find three full rectangles: a yellow one, a red
  one, and a blue one. Move each of them upward in the grid by as much
  as their height.
\item {\tt 5582e5ca}. Find two shapes on a 3x3 colored grid. Generate
  the same colored grid without the shapes. Actually, the task is to
  identify the majority color in the input grid. Even without a notion
  of counting and majority in the models, the MDL principle implicitly
  identifies the majority color as the one that compresses the most.
\end{itemize}
\HIDE{
  \item {\tt 05269061}. Find three shapes, and generate a three-color
  checker boards as a combination of three shapes with complex
  masks. One color is handled by the background color. Another color
  is handled by the top shape whose mask is a set of stripes. The last
  color is handled by the two bottom shapes, although a single one
  should be sufficient. The three input shapes are three stripes
  picked at random, one for each color. The learned model cannot
  predict for sure which color is where, so that it often fails on
  first trial. This task can actually not be genuinely understood by
  Version~2 models but interestingly, it could nonetheless be solved
  to some extent.}

For 11 training tasks, the approach was successful on all train
instances although it failed on the test instances. This means that a
working model was found but that it does not generalize to the test
instances. Here are some causes for the lack of generalization:
\begin{itemize}
\item wrong choice between constants and variables, and finding
  accidental arithmetic equalities (e.g., {\tt 0962bcdd});
\item wrong segmentation of shapes, e.g. prefering a full rectangle to
  an irregular shape (e.g., {\tt 1caeab9d});
\item the transformation should be performed several times in the test
  instance whereas once is enough in all train instances (e.g., {\tt
    29c11459});
\item there is an invariance by grid rotation or grid transposition
  (e.g., {\tt 4522001f}).
\end{itemize}


%%%
\section{Conclusion and Perspectives}
\label{conclu}

\bibliographystyle{splncs}
\bibliography{/local/ferre/recherche/biblio/bib}

\end{document}
